<<<<<<< HEAD
<h1 align="center">
  Open-Chinese-Llama2-Chat
</h1>
<p align="center">
  <font face="é»‘ä½“" color=orange size="6"> Llama-2ä¸­æ–‡é¢„è®­ç»ƒå¤§æ¨¡åž‹ </font>
</p>

## ç®€ä»‹
  æœ¬é¡¹ç›®åŸºäºŽLlama-2,é¢å‘ä¸åŒçš„ä¸šåŠ¡åœºæ™¯ä¸‹çš„åž‚ç›´é¢„æ–™, å®žçŽ°äº†åŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚


- [è®­ç»ƒæ•°æ®](#-æ•°æ®æ¥æº)
- [æ¨¡åž‹è®­ç»ƒ](#-æ¨¡åž‹è®­ç»ƒ)
  - [æ‰©å……è¯è¡¨](#-æ‰©å……è¯è¡¨)
  - [äºŒæ¬¡é¢„è®­ç»ƒ](#-é¢„è®­ç»ƒæ¨¡åž‹)
    - [stage_one](#-ç¬¬ä¸€é˜¶æ®µé¢„è®­ç»ƒ)
    - [stage_two](#-ç¬¬äºŒé˜¶æ®µé¢„è®­ç»ƒ)
  - [æ¨¡åž‹å¾®è°ƒ](#-æ¨¡åž‹å¾®è°ƒ)
    - [å¾®è°ƒè„šæœ¬](#å¾®è°ƒè„šæœ¬)
    - [æ•°æ®æ ¼å¼](#æ•°æ®æ ¼å¼)
  - [å¥–åŠ±æ¨¡åž‹](#-å¥–åŠ±æ¨¡åž‹)
    - [å¾®è°ƒè„šæœ¬](#å¾®è°ƒè„šæœ¬)
    - [æ•°æ®æ ¼å¼](#æ•°æ®æ ¼å¼)
  - [RLHFå¼ºåŒ–å­¦ä¹ ](#-æ¨¡åž‹å¾®è°ƒ)
- [æ¨¡åž‹æŽ¨ç†](#-æ¨¡åž‹æŽ¨ç†)
- [ðŸš€ æŽ¨ç†åŠ é€Ÿ](#-æŽ¨ç†åŠ é€Ÿ)
  - [vLLM](#vllm)
- [ç›¸å…³èµ„æ–™](#-å­¦ä¹ èµ„æ–™)
  - [Llamaç›¸å…³è®ºæ–‡](#llamaç›¸å…³è®ºæ–‡)
- [é—®é¢˜åé¦ˆ](#-é—®é¢˜åé¦ˆ)



## æ•°æ®æ¥æº

æˆ‘ä»¬è®¡åˆ’é€šè¿‡ä»¥ä¸‹æ•°æ®æ¥ä¼˜åŒ–Llama2çš„ä¸­æ–‡èƒ½åŠ›:

| ç±»åž‹                                                       | æè¿°                                                         |
| ---------------------------------------------------------- | ------------------------------------------------------------ |
| [Wikipedia](https://github.com/goldsmith/Wikipedia)        | ä¸­æ–‡Wikipediaçš„æ•°æ®                                          |
| [BelleGroup](https://huggingface.co/datasets/BelleGroup/train_1M_CN)                | çº¦100ä¸‡æ¡ç”±BELLEé¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æŒ‡ä»¤æ•°æ®ã€‚                                       |
| [ShareGPT-CN](https://huggingface.co/datasets/FreedomIntelligence/ShareGPT-CN) | ShareGPTä¸­æ–‡ç‰ˆ   |


### é¢„è®­ç»ƒæ¨¡åž‹

Llama2é¢„è®­ç»ƒæ¨¡åž‹åŒ…å«7Bã€13Bå’Œ70Bä¸‰ä¸ªç‰ˆæœ¬

| æ¨¡åž‹åç§°   | æ¨¡åž‹åŠ è½½åç§°             | ä¸‹è½½åœ°å€                                                     |
| ---------- | ------------------------- | ------------------------------------------------------------ |
| Llama2-7B  | meta-llama/Llama-2-7b-hf  | [æ¨¡åž‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-7b-hf)  |
| Llama2-13B | meta-llama/Llama-2-13b-hf | [æ¨¡åž‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-13b-hf) |
| Llama2-70B | meta-llama/Llama-2-70b-hf | [æ¨¡åž‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-70b-hf) |

### Chatæ¨¡åž‹

Llama2-Chatæ¨¡åž‹åŸºäºŽé¢„è®­ç»ƒæ¨¡åž‹è¿›è¡Œäº†ç›‘ç£å¾®è°ƒï¼Œå…·å¤‡æ›´å¼ºçš„å¯¹è¯èƒ½åŠ›

| æ¨¡åž‹åç§°        | æ¨¡åž‹åŠ è½½åç§°                  | ä¸‹è½½åœ°å€                                                     |
| --------------- | ------------------------------ | ------------------------------------------------------------ |
| Llama2-7B-Chat  | meta-llama/Llama-2-7b-chat-hf  | [æ¨¡åž‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) |
| Llama2-13B-Chat | meta-llama/Llama-2-13b-chat-hf | [æ¨¡åž‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) |
| Llama2-70B-Chat | meta-llama/Llama-2-70b-chat-hf | [æ¨¡åž‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) |


# æ¨¡åž‹è®­ç»ƒ

## æ‰©å……è¯è¡¨
ç”±äºŽåŽŸç‰ˆLLaMA2å’ŒLlamaçš„è¯è¡¨éƒ½åªæœ‰32000, å¯¹ä¸­æ–‡çš„æ”¯æŒä¸å¤ªå‹å¥½ï¼Œæ‰€ä»¥æœ¬é¡¹ç›®åœ¨LLaMA2çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å……äº†ä¸­æ–‡è¯è¡¨ã€‚
å‚è€ƒäº†Alpacaé¡¹ç›®ï¼Œé’ˆå¯¹Llama-2æ¨¡åž‹æ–°å¢žäº†æ–°ç‰ˆä¸­æ–‡è¯è¡¨å¹¶ä¸ŽåŽŸç‰ˆLLaMAæ¨¡åž‹çš„32Kè¯è¡¨è¿›è¡Œåˆå¹¶
æŽ’é™¤é‡å¤çš„tokenåŽï¼Œå¾—åˆ°çš„æœ€ç»ˆä¸­æ–‡LLaMAè¯è¡¨å¤§å°ä¸º55296ã€‚
- æä¾›ä»£ç merge_tokenizers.pyå‚è€ƒå‚è€ƒä»£ç æ‰©å……è¯è¡¨ã€‚è¯¥è„šæœ¬è¿è¡Œæ–¹å¼å¦‚ä¸‹ï¼š

```bash
  python merge_tokenizers.py \
  --llama_tokenizer_dir ./tokenizer_model/ \
  --chinese_sp_model_file ./chinese_sp.model
```
- llama_tokenizer_dirï¼šåŽŸllamaè¯è¡¨è·¯å¾„ç›®å½•
- chinese_sp.modelï¼šæ–°è¯è¡¨æ–‡ä»¶

## é¢„è®­ç»ƒæ¨¡åž‹
é¢„è®­ç»ƒæœ¬é¡¹ç›®åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µä»…è®­ç»ƒembeddingå±‚ï¼Œç›®çš„æ˜¯è®©æ‰©å……çš„ä¸­æ–‡è¯è¡¨æ›´å¥½çš„é€‚åº”æ¨¡åž‹ã€‚ä½†æ˜¯å¼Šç«¯æ˜¯è¯¥é˜¶æ®µæ”¶æ•›é€Ÿé€’ä¼šéžå¸¸æ…¢ï¼Œ
å¦‚æžœä¸æ˜¯æœ‰ç‰¹åˆ«å……è£•çš„æ—¶é—´å’Œè®¡ç®—èµ„æºï¼Œä¹Ÿå¯ä»¥è·³è¿‡è¯¥é˜¶æ®µï¼Œç›´æŽ¥è¿›è¡Œç¬¬äºŒé˜¶æ®µçš„é¢„è®­ç»ƒã€‚

- ##ç¬¬ä¸€é˜¶æ®µé¢„è®­ç»ƒ
    ç¬¬ä¸€é˜¶æ®µçš„é¢„è®­ç»ƒä»£ç ï¼Œæœ¬é¡¹ç›®æ˜¯å”¯ä¸€å¼€æºå‡ºæ¥çš„ï¼Œè¿è¡Œå¦‚ä¸‹å‘½ä»¤ä¾›å‚è€ƒï¼š
    ```bash
    CUDA_VISIBLE_DEVICES=2,3 torchrun --master_port 29510 --nproc_per_node=2 run_clm.py \
        --model_name_or_path ./Llama-2-13b-hf/ \
        --train_file ./wiki_embeddings_data.txt \
        --per_device_train_batch_size 1 \
        --per_device_eval_batch_size 1 \
        --do_train \
        --do_eval False \
        --output_dir ./Llama-2-13b-embeddings_v1/ \
        --gradient_accumulation_steps 16 \
        --gradient_checkpointing True \
        --block_size 4096 \
        --logging_steps 2 \
        --save_steps=500 \
        --num_train_epochs 1
    ```
  è¿™ä¸€é˜¶æ®µçš„çš„æ•°æ®é›†ç¤ºä¾‹å¦‚ä¸‹ï¼š

- ##ç¬¬äºŒé˜¶æ®µé¢„è®­ç»ƒ
   ç¬¬äºŒé˜¶æ®µé¢„è®­ç»ƒä½¿ç”¨LoRAæŠ€æœ¯(è®ºæ–‡â€œ[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)â€ä»¥åŠæºç [LoRA](https://github.com/microsoft/LoRA))ï¼Œè®­ç»ƒembeddingçš„åŒæ—¶ä¹Ÿæ›´æ–°LoRAå‚æ•°ã€‚
   è„šæœ¬è¿è¡Œå¦‚ä¸‹å‘½ä»¤ä¾›å‚è€ƒï¼š
   ```bash
    CUDA_VISIBLE_DEVICES=2,3 torchrun --master_port 29510 --nproc_per_node=2 run_clm_pt_with_peft.py \
    --deepspeed ds_zero2_no_offload.json \
    --model_name_or_path /data_hxs/Llama-2-13b-embeddings_v1/ \
    --tokenizer_name_or_path /data_hxs/Llama-2-13b-embeddings_v1/ \
    --dataset_dir /data_hxs/data_pt/ \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --do_train \
    --fp16 \
    --num_train_epochs 3 \
    --lr_scheduler_type cosine \
    --learning_rate 2e-4 \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 3 \
    --save_steps 500 \
    --gradient_accumulation_steps 16 \
    --preprocessing_num_workers 8 \
    --block_size 4096 \
    --output_dir /data_hxs/Llama-2-13b-pt_v1/ \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank 64 \
    --lora_alpha 128 \
    --lora_dropout 0.05 \
    --modules_to_save "embed_tokens,lm_head" \
    --torch_dtype float16 \
    --gradient_checkpointing True \
    --ddp_find_unused_parameters False
    ```

    #### åˆå¹¶è„šæœ¬    
    è®­ç»ƒå®ŒæˆåŽï¼Œå¾—åˆ°çš„æ˜¯Loraçš„æƒé‡ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆå¹¶æˆhfæ ¼å¼çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå‚è€ƒè„šæ­¥å‘½ä»¤å¦‚ä¸‹ï¼š
    
      ```bash
      python merge_llama_with_chinese_lora.py \
        --base_model /data_hxs/output/llama2-13b/ \
        --lora_model /data_hxs/output/llama2-13b-lora/ \
        --output_type huggingface \
        --output_dir /data_hxs/output/llama2-13b_merge
      ```
    è¿™ä¸€é˜¶æ®µçš„çš„æ•°æ®é›†ç¤ºä¾‹å¦‚ä¸‹ï¼š


## æ¨¡åž‹å¾®è°ƒ
SFTæœ‰ç›‘ç£å¾®è°ƒï¼Œæž„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨ä¸Šä¸€æ­¥é¢„è®­ç»ƒæ¨¡åž‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œç›®çš„æ˜¯å¯¹é½æŒ‡ä»¤æ„å›¾ã€‚

  #### å¾®è°ƒè„šæœ¬
  è„šæœ¬è¿è¡Œå¦‚ä¸‹å‘½ä»¤ä¾›å‚è€ƒï¼š
  ```bash
  CUDA_VISIBLE_DEVICES=2,3,4,5 torchrun --master_port 29510 --nproc_per_node=4 run_clm_sft_with_peft.py \
      --deepspeed ./ds_zero2_no_offload.json \
      --model_name_or_path path/to/model \
      --dataset_dir path/to/sft/data/dir \
      --validation_split_percentage 0.1 \
      --per_device_train_batch_size 2 \
      --per_device_eval_batch_size 2 \
      --do_train \
      --do_eval \
      --seed $RANDOM \
      --fp16 \
      --num_train_epochs 3 \
      --lr_scheduler_type cosine \
      --learning_rate 2e-4 \
      --warmup_ratio 0.03 \
      --weight_decay 0 \
      --logging_strategy steps \
      --logging_steps 10 \
      --save_strategy steps \
      --save_total_limit 3 \
      --evaluation_strategy steps \
      --eval_steps 250 \
      --save_steps 500 \
      --gradient_accumulation_steps 16 \
      --preprocessing_num_workers 8 \
      --max_seq_length 512 \
      --output_dir path/to/output \
      --overwrite_output_dir \
      --ddp_timeout 30000 \
      --logging_first_step True \
      --lora_rank 8 \
      --lora_alpha 32 \
      --lora_dropout 0.05 \
      --torch_dtype float16 \
      --gradient_checkpointing True \
      --ddp_find_unused_parameters False
  ```

  - è®­ç»ƒå®ŒæˆåŽï¼Œå¾—åˆ°çš„æ˜¯Loraçš„æƒé‡ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆå¹¶æˆhfæ ¼å¼çš„æ¨¡åž‹æ–‡ä»¶ï¼ŒåŒä¸Šã€‚

  #### æ•°æ®æ ¼å¼
  è¿™ä¸€é˜¶æ®µçš„çš„æ•°æ®é›†ç¤ºä¾‹å¦‚ä¸‹, å¦‚ä¸‹æ ¼å¼çš„jsonæ–‡ä»¶ï¼š
  ```
  [
    {
      "instruction": "æå‡ºä¸€ç§å¯¹ç»™å®šæ•°æ®è¿›è¡Œåˆ†ç±»çš„æ–¹æ³•ã€‚",
      "input": "ä¸åŒç±»åž‹é±¼çš„åˆ—è¡¨",
      "output": "å¯ä»¥æ ¹æ®å®ƒä»¬çš„æ –æ¯åœ°å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œä¾‹å¦‚æ·¡æ°´é±¼ã€æµ·æ°´é±¼å’Œæ²³é±¼ã€‚ä¹Ÿå¯ä»¥æ ¹æ®å®ƒä»¬çš„åœ°ç†ä½ç½®è¿›è¡Œåˆ†ç±»ï¼Œä¾‹å¦‚ç”Ÿæ´»åœ¨æ¸©å¸¦æˆ–çƒ­å¸¦åœ°åŒºçš„é±¼ç±»ã€‚æ­¤å¤–ï¼Œæ•°æ®è¿˜å¯ä»¥æ ¹æ®å®ƒä»¬é£Ÿç”¨çš„é£Ÿç‰©ç±»åž‹è¿›è¡Œåˆ†ç±»ï¼Œä¾‹å¦‚é£Ÿè‰é±¼ã€æ‚é£Ÿé±¼æˆ–é£Ÿè‚‰é±¼ã€‚"
    }
  ]
  ```

## å¥–åŠ±æ¨¡åž‹
æž„é€ äººç±»åå¥½æŽ’åºæ•°æ®é›†ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡åž‹ï¼Œç”¨æ¥å¯¹é½äººç±»åå¥½ã€‚
  
  #### å¾®è°ƒè„šæœ¬
  - è„šæœ¬è¿è¡Œå¦‚ä¸‹å‘½ä»¤ä¾›å‚è€ƒï¼š
    ```bash
      CUDA_VISIBLE_DEVICES=4 python rm.py \
        --model_type llama \
        --model_name_or_path ./Llama-2-13b-hf/ \
        --tokenizer_name_or_path ./tokenizer_path/ \
        --train_file_dir ./reward \
        --validation_file_dir ./reward \
        --per_device_train_batch_size 2 \
        --per_device_eval_batch_size 2 \
        --do_train \
        --use_peft True \
        --seed 42 \
        --max_train_samples 1000 \
        --max_eval_samples 10 \
        --num_train_epochs 1 \
        --learning_rate 2e-5 \
        --warmup_ratio 0.05 \
        --weight_decay 0.001 \
        --logging_strategy steps \
        --logging_steps 1 \
        --evaluation_strategy no \
        --save_steps 500 \
        --save_strategy steps \
        --save_total_limit 3 \
        --max_source_length 256 \
        --max_target_length 256 \
        --output_dir ./Llama-2-13b-rw_v1 \
        --overwrite_output_dir \
        --ddp_timeout 30000 \
        --logging_first_step True \
        --target_modules all \
        --lora_rank 8 \
        --lora_alpha 16 \
        --lora_dropout 0.05 \
        --torch_dtype float32 \
        --device_map auto \
        --report_to tensorboard \
        --ddp_find_unused_parameters False \
        --remove_unused_columns False \
        --gradient_checkpointing True
    ```
    #### æ•°æ®æ ¼å¼
      è¿™ä¸€é˜¶æ®µçš„çš„æ•°æ®é›†ç¤ºä¾‹å¦‚ä¸‹, å¦‚ä¸‹æ ¼å¼çš„jsonæ–‡ä»¶ï¼š
      ```
      {"question": "å­©å­æ„Ÿç»Ÿå¤±è°ƒï¼Œå­¦ä¹ ä¸è¡Œæ€Žä¹ˆåŠžï¼Ÿï¼Œå­©å­è¯´è¯æ™šï¼Œèµ°è·¯æ™šï¼ŒçŽ°åœ¨å°å­¦äºŒå¹´çº§ï¼Œå­¦ä¹ è·Ÿä¸ä¸Šï¼Œç†è§£åŠ›å·®ï¼Œè¿åŠ¨åè°ƒæ€§å·®ï¼Œå®¶é‡Œå¾ˆç€æ€¥ï¼Œä¸çŸ¥æ€Žä¹ˆåŠžã€‚", "response_chosen": "ç—…æƒ…åˆ†æžï¼šä½ å¥½!å­©å­è¯´è¯æ™šï¼Œèµ°è·¯ä¹Ÿæ™šï¼Œå¾ˆå¯èƒ½æ˜¯å¤§è„‘æœ¬èº«å‘è‚²ä¸å¥½å¼•èµ·çš„å‘è‚²è¿Ÿç¼“ã€‚è€Œäº”å²æ—¶æ‰€è‡´çš„æ™ºåŠ›æ£€æŸ¥ä¸ºä¸´ç•ŒèŒƒå›´ï¼Œé‚£å°±æ˜¯è¯´å­©å­çš„æ™ºå•†æ˜¯æœ‰é—®é¢˜çš„ï¼Œä¹Ÿåº”è€ƒè™‘ä¸Žå¤§è„‘å‘è‚²ä¸å¥½æœ‰å…³ã€‚æŒ‡å¯¼æ„è§ï¼šäººçš„å¤§è„‘åœ¨å¤´ä¸€å¹´å‘è‚²æœ€å¿«ï¼Œå¯å¡‘æ€§æœ€å¼ºï¼Œåœ¨å¤´ä¸‰å¹´å¯å¡‘æ€§è¿˜æ˜¯å¯ä»¥çš„ï¼Œè¶…è¿‡ä¸‰å²å†è¿›è¡Œè®­ç»ƒï¼Œæ•ˆæžœå°±ä¸æ€Žä¹ˆå¥½äº†ã€‚å»ºè®®å†ç»™å­©å­åšä¸€åšæ™ºåŠ›æµ‹è¯•ï¼Œå¦‚æžœå­©å­çš„æ™ºå•†è¿˜æ˜¯åœ¨ä¸´ç•ŒèŒƒå›´ï¼Œé‚£å°±è¦è€ƒè™‘è®©å­©å­åŽ»ç‰¹æ®Šå­¦æ ¡è¿›è¡Œåº·å¤è®­ç»ƒï¼Œè€Œä¸æ˜¯ç»§ç»­åœ¨æ™®é€šå°å­¦å°±è¯»ï¼Œå¦åˆ™å¯¹å­©å­æ¥è¯´ï¼Œå°±æ˜¯å¼ºäººæ‰€éš¾äº†ã€‚å¸Œæœ›è‡ªå·±çš„å­©å­èƒ½èªæ˜Žï¼Œè¿™æ˜¯æ¯ä¸ªå®¶é•¿éƒ½ä¼šæœ‰çš„å¿ƒæ„¿ï¼Œä½†å¦‚æžœå­©å­è‡ªèº«çš„æ¡ä»¶å°±æ˜¯ä¸èƒ½è·Ÿä¸ŠåŒé¾„å­©å­ï¼Œé‚£å®¶é•¿ä¹Ÿè¦é¢å¯¹è¿™ä¸ªäº‹å®žçš„ï¼Œå¯¹å—ï¼ŸåŒ»ç”Ÿè¯¢é—®ï¼š", "response_rejected": "å»ºè®®å®¶é•¿å…ˆå¸¦å­©å­åŽ»æ­£è§„åŒ»é™¢åšå…¨é¢æ£€æŸ¥ä»¥ç¡®å®šç—…å› å’Œç—…æƒ…ä¸¥é‡ç¨‹åº¦ï¼›åŒæ—¶å¯ä»¥è¿›è¡Œç‰©ç†æ²»ç–—ã€åº·å¤è®­ç»ƒç­‰è¾…åŠ©æ²»ç–—æ–¹æ³•ã€‚"}
      ```
## RLHF å¼ºåŒ–å­¦ä¹ 
åŸºäºŽäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)ï¼Œç”¨å¥–åŠ±æ¨¡åž‹æ¥è®­ç»ƒSFTæ¨¡åž‹ï¼Œç”Ÿæˆæ¨¡åž‹ä½¿ç”¨å¥–åŠ±æˆ–æƒ©ç½šæ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„æ–‡æœ¬
æ•¬è¯·æœŸå¾…ï¼Œä¸ä¹…åŽå°†å…¬å¸ƒã€‚


# æ¨¡åž‹æŽ¨ç†
- æ¨¡åž‹è°ƒç”¨ä»£ç ç¤ºä¾‹
  å¦‚æžœä½ ä¸‹è½½çš„æ˜¯å®Œæ•´ç‰ˆæƒé‡ï¼Œæˆ–è€…ä¹‹å‰å·²æ‰§è¡Œäº†åˆå¹¶è„šæœ¬å°†LoRAæƒé‡ä¸ŽåŽŸç‰ˆLlama-2åˆå¹¶ï¼Œå¯ç›´æŽ¥åŠ è½½å®Œæ•´ç‰ˆæ¨¡åž‹ã€‚
  ```bash
  CUDA_VISIBLE_DEVICES=4 python inference/inference_hf.py \
      --base_model /data_hxs/code/Chinese-LLaMAA/Llama-2-13b-hf/ \
      --interactive \
      --with_prompt \
      --gpus 4
  ```

### Llamaç›¸å…³è®ºæ–‡
* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)

## é—®é¢˜åé¦ˆ
å¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨Issueä¸­æäº¤ï¼Œåœ¨æäº¤é—®é¢˜ä¹‹å‰ï¼Œè¯·å…ˆæŸ¥é˜…ä»¥å¾€çš„issueæ˜¯å¦èƒ½è§£å†³ä½ çš„é—®é¢˜ã€‚
=======
# open-llama2
ä»Žé¢„è®­ç»ƒåˆ°å¼ºåŒ–å­¦ä¹ çš„ä¸­æ–‡llama2
>>>>>>> 441f52eeab95af54fef3af1b1f93a20c2547d75d
